library(tidyverse)
library(lme4)
library(lmerTest)
library(emmeans)
library(fitdistrplus)
library(brms)
library(afex)

# Region 4 ####
# Region 4 is the critical region (the question)
# Read in first pass data
data <- read_csv("FPs2.csv")
# condition labels, 1 = prediction facilitated, 2 = prediction unfacilitated 
data$Condition <- recode(data$Condition, "1" = "Facilitated", "2" = "Unfacilitated")
#make Condition a factor
data$Condition <- as.factor(data$Condition)
# Filter by non-zero values for R4 (remove all cases where there is no fixation on R4)
data <- data %>% filter(R4 != 0)


# Visualise ####

# flat violin code
source("https://gist.githubusercontent.com/benmarwick/2a1bb0133ff568cbe28d/raw/fb53bd97121f7f9ce947837ef1a4c65a73bffb3f/geom_flat_violin.R")

# preparing data for visualisation
data_long <- read_csv("FPs2.csv") %>%
  gather(Region, RT, R1:R6, factor_key=TRUE) %>%  # making Reading Time a single column
  filter(Region == 'R4' | Region == 'R5' ) %>%  # filtering out all regions other than R4 and R5
  filter(RT != 0)  # Filter by non-zero values
data_long$Participant <- as.factor(data_long$Participant)  # making participant a factor
data_long$Condition <- as.factor(data_long$Condition)  # making condition a factor
data_long$Region <- as.factor(data_long$Region)  # making region a factor
data_long$Condition <- recode(data_long$Condition, "1" = "Facilitated", "2" = "Unfacilitated")  # naming levels of condition
data_long$Region <- recode(data_long$Region, "R4" = "Question", "R5" = "Reply")  # naming regions


theme = theme(
  text = element_text(size = 14),
  axis.title.x = element_blank(),
  axis.title.y = element_text(size = 16),
  axis.text = element_text(size = 12),
  legend.title=element_text(size=14),
  legend.text=element_text(size=14),
  panel.spacing=unit(0,"cm"),
  panel.background = element_rect(fill = "white", colour = "white"),
  panel.grid.major.y = element_line(linetype= 'solid', colour = "grey"),
  panel.border = element_rect(fill = NA, colour = "grey", size = 1),
  strip.placement = "outside",
  strip.text = element_text(size = 14),
  strip.background.x = element_blank())

data_long %>% 
  ggplot(aes(x = Condition, y = RT)) +
  geom_flat_violin(position = position_nudge(x = .13, y = 0), alpha = .7) +
  geom_jitter(alpha = .1, width = .1) +
  geom_boxplot(width = .1, guides = FALSE, outlier.shape = NA, alpha = 0.5, position=position_dodge2()) +
  facet_grid(~ Region, switch="x") +
  ylab("First Pass Reading Time (ms)") +
  scale_y_continuous(breaks = seq(0, 5000, 1000), expand = expand_scale(mult = c(0, 0.05))) +
  theme

# exported with 800x500 aspect ratio

# so it looks like the facilitated condition is has slightly faster reading times, on average

#descriptive statistics
data %>% 
  group_by(Condition) %>%
  summarise(mean(R4), sd(R4))
# the descriptive statistics also show a shorter reading times for the facilitated condition

# end visualise ####

# Model assuming normality of residuals - singular fit error with more complex models
model.null <- lmer(R4 ~ (1  | Participant) + (1 | Item), data) 
model <- lmer(R4 ~ Condition + (1 | Participant) + (1 | Item), data) 
summary(model)
# the p value is significant

# the anova comparing the null model with the alternative shows no siginificant difference
# the BIC values suggest that the alternative is more likely
anova(model, model.null)


# plotting the normality of the residuals
qqnorm(residuals(model.null))
qqline(residuals(model.null))
qqnorm(residuals(model))
qqline(residuals(model))
# the residuals don't appear normally distributed

# testing the linearity of residuals
plot(fitted(model), residuals(model))

# what is the distribution of the data?
descdist(data$R4)
# it looks closest to lognormal distribution

# Model with log transform - singular fit with more complex structures or when slope is on participant
model <- lmer(log(R4) ~ Condition + (1 + Condition | Participant) + (1 + Condition | Item), data) # singular fit
null.model <- lmer(log(R4) ~ (1 + Condition | Participant) + (1 + Condition | Item), data) # singular fit
model <- lmer(log(R4) ~ Condition + (1 + Condition | Participant) + (1 | Item), data) # singular fit
null.model <- lmer(log(R4) ~ (1 + Condition | Participant) + (1 | Item), data) # singular fit

model <- lmer(log(R4) ~ Condition + (1 | Participant) + (1 + Condition | Item), data) # converges
null.model <- lmer(log(R4) ~ (1 | Participant) + (1 + Condition | Item), data) # converges

summary(model)
anova(model, null.model)
# condition is  not significant
# lower BIC is better - the full model hasn't improved against the null model given the penalty that it imposes
MLmodel <- lmer(log(R4) ~ Condition + (1 | Participant) + (1 + Condition | Item), data, REML=FALSE) 
MLnull.model <- lmer(log(R4) ~ (1 | Participant) + (1 + Condition | Item), data, REML=FALSE)
BIC(MLmodel)
BIC(MLnull.model)
BICdiff <- (BIC(MLnull.model) - BIC(MLmodel))
BICdiff
exp(BICdiff/2)
# and then Bayes factor in favour of null (for illustration):
BICdiff <- (BIC(MLmodel) - BIC(MLnull.model))
BICdiff
exp(BICdiff/2)

#plotting the residuals again

hist(residuals(model))
qqnorm(residuals(model))
qqline(residuals(model))
plot(fitted(model), residuals(model))

# Bayes generalised linear multivariate multilevel models
model_brms <- brm(log(R4) ~ Condition + (1 + Condition | Participant) + (1 + Condition | Item), data, save_all_pars = FALSE) # converges
summary(model_brms)

# Region 5 ####

# Read in first pass data
data <- read_csv("FPs2.csv")
# condition labels, 1 = prediction facilitated, 2 = prediction unfacilitated 
data$Condition <- recode(data$Condition, "1" = "facilitated", "2" = "unfacilitated")
data$Condition <- as.factor(data$Condition)
data <- data %>% filter(R5 != 0)

# Model assuming normality of residuals - singular fit error with more complex models
model.null <- lmer(R5 ~ (1 | Participant) + (1 | Item), data) 
model <- lmer(R5 ~ Condition + (1 | Participant) + (1 | Item), data) 
summary(model)
anova(model, model.null)

qqnorm(residuals(model))
qqline(residuals(model))
plot(fitted(model), residuals(model))

descdist(data$R5)
# closest to lognormal distribution

# Model with log transform - fits with maximal random effects structure
model <- lmer(log(R5) ~ Condition + (1 + Condition | Participant) + (1 + Condition | Item), data) # converges
null.model <- lmer(log(R5) ~ (1 + Condition | Participant) + (1 + Condition | Item), data) # converges
summary(model)
anova(null.model, model)
# model and t-test are both significant
MLmodel <- lmer(log(R5) ~ Condition + (1 + Condition | Participant) + (1 + Condition | Item), data, REML=FALSE) 
MLnull.model <- lmer(log(R5) ~ (1 + Condition | Participant) + (1 + Condition | Item), data, REML=FALSE) 
BIC(MLmodel)
BIC(MLnull.model)
BICdiff <- (BIC(MLnull.model) - BIC(MLmodel))
BICdiff
exp(BICdiff/2)

qqnorm(residuals(model))
qqline(residuals(model))
plot(fitted(model), residuals(model))
# this fit is a bit better

# Bayes generalised linear multivariate multilevel models
model_brms <- brm(log(R5) ~ Condition + (1 + Condition | Participant) + (1 + Condition | Item), data, save_all_pars = FALSE) # converges
summary(model_brms)
